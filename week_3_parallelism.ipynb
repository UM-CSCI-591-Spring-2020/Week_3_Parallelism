{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data parallelism: Exercise\n",
    "\n",
    "For this exercise we will be build upon last week's vanilla gradient descent example. Included in the next codebox are functions to perform feedforward and backprop on a single minibatch. The computeMinibatchGradientsTuple() function is the same as the computeMinibatchGradients() function, but its inputs in a single tuple will make using Python's ThreadPool easier later on.\n",
    "\n",
    "You donâ€™t need to do modify this first block of code. \n",
    "\n",
    "If you do not have scikit-learn then you can get it here: https://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X[::5]\n",
    "y = y.astype(int)[::5]\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "\n",
    "# Here we specify the size of our neural network.\n",
    "# We are mapping from 784 to 10 with 256 hiden layer nodes.\n",
    "\n",
    "m = len(X)\n",
    "n_0 = 784\n",
    "n_1 = 256\n",
    "N = 10\n",
    "\n",
    "\n",
    "# Function to convert categorical labels into one-hot matrix.\n",
    "def convert_to_one_hot(y, n_classes):\n",
    "    T = np.zeros((y.shape[0], n_classes))\n",
    "    for t, yy in zip(T, y):\n",
    "        t[yy] = 1\n",
    "    return T\n",
    "\n",
    "\n",
    "# Convert the data to one hot notation\n",
    "one_hot_y_actual = convert_to_one_hot(y, N)\n",
    "one_hot_y_test = convert_to_one_hot(y_test, N)\n",
    "\n",
    "\n",
    "# Sigmoid function (activation)\n",
    "def sigmoid(a):\n",
    "    return 1. / (1 + np.exp(-a))\n",
    "\n",
    "\n",
    "# Softmax function (final layer for classification)\n",
    "def softmax(A):\n",
    "    numerator = np.exp(A)\n",
    "    denominator = numerator.sum(axis=1)\n",
    "    return numerator / denominator[:, np.newaxis]\n",
    "\n",
    "\n",
    "# Categorical cross-entropy\n",
    "def L(T, S, W1, W2, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    return -1. / len(T) * np.sum(T * np.log(S)) + np.sum(0.5 * alpha_1 * W1 ** 2) + np.sum(0.5 * alpha_2 * W2 ** 2)\n",
    "\n",
    "\n",
    "# Run the neural network forward, given some weights and biases\n",
    "def feedforward(X, W1, W2, b1, b2):\n",
    "    # Feedforward\n",
    "    A1 = X @ W1 + b1\n",
    "    Z1 = sigmoid(A1)\n",
    "    A2 = Z1 @ W2 + b2\n",
    "    y_pred = softmax(A2)\n",
    "    return y_pred, Z1\n",
    "\n",
    "\n",
    "# Compute the neural network gradients using backpropagation\n",
    "def backpropogate(y_pred, Z1, X, y_obs, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    # Backpropogate\n",
    "    delta_2 = (1. / len(y_pred)) * (y_pred - y_obs)\n",
    "    grad_W2 = Z1.T @ delta_2 + alpha_2 * W2\n",
    "    grad_b2 = delta_2.sum(axis=0)\n",
    "\n",
    "    delta_1 = delta_2 @ W2.T * Z1 * (1 - Z1)\n",
    "    grad_W1 = X.T @ delta_1 + alpha_1 * W1\n",
    "    grad_b1 = delta_1.sum(axis=0)\n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "\n",
    "def mini_batch(x_sample, y_sample, start_batch_size):\n",
    "    \"\"\"\n",
    "    Takes a copy of x_sample and y_sample and returns mini batch matrices of both and number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    # Batches must divide evenly into total number of samples for numpy arrays to be happy.\n",
    "    # Gets number of bathes by finding next smallest number that evenly divides\n",
    "    num_batches = start_batch_size\n",
    "    while len(x_sample) % num_batches != 0:\n",
    "        num_batches -= 1\n",
    "\n",
    "    # randomly shuffle indices\n",
    "    np.random.seed(42)\n",
    "    random_indices = np.random.choice(range(len(x_sample)), len(x_sample), replace=False)\n",
    "\n",
    "    # instantiate lists to hold batches\n",
    "    x_list = [[] for i in range(num_batches)]\n",
    "    y_list = [[] for i in range(num_batches)]\n",
    "\n",
    "    # populate batches matrix with random mini batch indices\n",
    "    for i in range(len(x_sample)):\n",
    "\n",
    "        x_list[i // 105].append(x_sample[random_indices[i]])\n",
    "        y_list[i // 105].append(y_sample[random_indices[i]])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    x_batch = np.array(x_list)\n",
    "    y_batch = np.array(y_list)\n",
    "\n",
    "    return x_batch, y_batch, num_batches, num_batches\n",
    "\n",
    "\n",
    "#computes the gradients of a single minibatch\n",
    "def computeMinibatchGradients(W1, W2, b1, b2, x_batch, y_batch):\n",
    "    y_pred, Z1 = feedforward(x_batch, W1, W2, b1, b2)\n",
    "    \"\"\"\n",
    "    These are your gradients with respect to weight matrices W1 and W2 \n",
    "    as well as your biases b1 and b2\n",
    "    \"\"\"\n",
    "    grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batch, y_batch)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "#computes the gradients of a single minibatch\n",
    "def computeMinibatchGradientsTuple(inputTuple):\n",
    "    W1, W2, b1, b2, x_batch, y_batch = inputTuple\n",
    "    y_pred, Z1 = feedforward(x_batch, W1, W2, b1, b2)\n",
    "    \"\"\"\n",
    "    These are your gradients with respect to weight matrices W1 and W2 \n",
    "    as well as your biases b1 and b2\n",
    "    \"\"\"\n",
    "    grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batch, y_batch)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "This next codebox should look familiar; it performs vanilla gradient descent. You don't need to change this codebox, either. Run this, and notice that it now also prints out the time taken to evaluate each epoch. We'll use these times to evaluate how much of a speedup data parallelism will give us in a simple multithreading environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.169349 Accuracy 0.610571 time taken 759 ms\n",
      "Epoch 10 Loss 0.776896 Accuracy 0.889714 time taken 697 ms\n",
      "Epoch 20 Loss 0.458293 Accuracy 0.911143 time taken 715 ms\n",
      "Epoch 30 Loss 0.335383 Accuracy 0.917429 time taken 831 ms\n",
      "Epoch 40 Loss 0.269738 Accuracy 0.920857 time taken 704 ms\n",
      "Epoch 50 Loss 0.228180 Accuracy 0.923714 time taken 776 ms\n",
      "Epoch 60 Loss 0.199441 Accuracy 0.924571 time taken 727 ms\n",
      "Epoch 70 Loss 0.178256 Accuracy 0.926000 time taken 690 ms\n",
      "Epoch 80 Loss 0.161905 Accuracy 0.926000 time taken 851 ms\n",
      "Epoch 90 Loss 0.148855 Accuracy 0.925714 time taken 822 ms\n",
      "Epoch 100 Loss 0.138163 Accuracy 0.926857 time taken 835 ms\n",
      "Epoch 110 Loss 0.129164 Accuracy 0.926286 time taken 739 ms\n",
      "Epoch 120 Loss 0.121513 Accuracy 0.928857 time taken 716 ms\n",
      "Epoch 130 Loss 0.114957 Accuracy 0.929714 time taken 710 ms\n",
      "Epoch 140 Loss 0.109212 Accuracy 0.930571 time taken 694 ms\n",
      "Epoch 150 Loss 0.104233 Accuracy 0.930857 time taken 698 ms\n",
      "Epoch 160 Loss 0.099832 Accuracy 0.931429 time taken 692 ms\n",
      "Epoch 170 Loss 0.095905 Accuracy 0.932000 time taken 712 ms\n",
      "Epoch 180 Loss 0.092406 Accuracy 0.932571 time taken 814 ms\n",
      "Epoch 190 Loss 0.089245 Accuracy 0.932571 time taken 751 ms\n",
      "Epoch 200 Loss 0.086360 Accuracy 0.933143 time taken 699 ms\n",
      "Epoch 210 Loss 0.083693 Accuracy 0.934857 time taken 730 ms\n",
      "Epoch 220 Loss 0.081272 Accuracy 0.934571 time taken 774 ms\n",
      "Epoch 230 Loss 0.078996 Accuracy 0.934857 time taken 774 ms\n",
      "Epoch 240 Loss 0.076915 Accuracy 0.935143 time taken 896 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vanilla Gradient Descent\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "# Hyper Parameters\n",
    "eta = 1e-3\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "vanilla_loss = []\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "    epochStartTime = time.time()\n",
    "    \n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(0, num_batches):\n",
    "        \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = computeMinibatchGradients(W1, W2, b1, b2, x_batches[j], y_batches[j])\n",
    "        '''\n",
    "        use the gradients to update weights and biases\n",
    "        '''\n",
    "        W1 -= eta * grad_W1\n",
    "        W2 -= eta * grad_W2\n",
    "        b1 -= eta * grad_b1\n",
    "        b2 -= eta * grad_b2\n",
    "\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    vanilla_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "    \n",
    "    #find the time taken to compute the epoch\n",
    "    epochTimeTaken = (time.time() - epochStartTime)*1000\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f time taken %d ms\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc, epochTimeTaken))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Vanilla Gradient Descent with Data Parallelism\n",
    "\n",
    "Now that we have some baseline timings, we're going to be updating this example to employ data parallelism. The Ben-Nun et.al. paper mainly focuses on parallelism in a distributed computing environment, but using a library like MPI for distributed parallelism would be well outside the scope of these assignments, so we're going to using Python's multiprocessing package to perform data parallelism with a ThreadPool\n",
    "\n",
    "First, read the documentation on python's Pool class, located here:\n",
    "https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing\n",
    "\n",
    "We're going to be using the Pool's faster (and less documented) cousin, the ThreadPool. The Pool and ThreadPool have the same interface, but while Pool uses a single thread, trading it between the pool's workers, the ThreadPool actually spins up multiple instances of the Python interpreter in different threads to perform true parallel computation.\n",
    "\n",
    "The next codeblock uses the ThreadPool's map function to give each process a different minibatch in parallel. \n",
    "\n",
    "\n",
    "1.\n",
    "On line 60, use the ThreadPool's map function to parallelize the gradient calculation for each of the parallel batches.\n",
    "\n",
    "2.\n",
    "We will need to average the gradients returned from each parallel batch in order to perform gradient descent, but the thread pool returns a list of the list of each batch's gradients. To make averaging the gradients easier, line 58 uses the zip function to make a new list such that the first element in the list contains all the W1 gradients, the second element contains all the W2 gradients, etc. On lines 59-62, use the np.mean function to average all W1, W2, b1, and b2 gradients, and use those averages to update W1, W2, b1, and b2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.300931 Accuracy 0.092571 time taken 1264 ms\n",
      "Epoch 10 Loss 2.231029 Accuracy 0.427429 time taken 979 ms\n",
      "Epoch 20 Loss 2.114058 Accuracy 0.649714 time taken 959 ms\n",
      "Epoch 30 Loss 1.974481 Accuracy 0.651429 time taken 970 ms\n",
      "Epoch 40 Loss 1.833836 Accuracy 0.670857 time taken 979 ms\n",
      "Epoch 50 Loss 1.701170 Accuracy 0.694000 time taken 952 ms\n",
      "Epoch 60 Loss 1.579504 Accuracy 0.718857 time taken 1025 ms\n",
      "Epoch 70 Loss 1.469206 Accuracy 0.744000 time taken 1052 ms\n",
      "Epoch 80 Loss 1.369618 Accuracy 0.769143 time taken 949 ms\n",
      "Epoch 90 Loss 1.279815 Accuracy 0.794286 time taken 1547 ms\n",
      "Epoch 100 Loss 1.198874 Accuracy 0.812571 time taken 1002 ms\n",
      "Epoch 110 Loss 1.125911 Accuracy 0.833143 time taken 1093 ms\n",
      "Epoch 120 Loss 1.060112 Accuracy 0.845714 time taken 1103 ms\n",
      "Epoch 130 Loss 1.000736 Accuracy 0.857429 time taken 972 ms\n",
      "Epoch 140 Loss 0.947098 Accuracy 0.868857 time taken 1225 ms\n",
      "Epoch 150 Loss 0.898566 Accuracy 0.877714 time taken 1047 ms\n",
      "Epoch 160 Loss 0.854572 Accuracy 0.884286 time taken 1027 ms\n",
      "Epoch 170 Loss 0.814600 Accuracy 0.888000 time taken 990 ms\n",
      "Epoch 180 Loss 0.778190 Accuracy 0.892000 time taken 1074 ms\n",
      "Epoch 190 Loss 0.744937 Accuracy 0.896000 time taken 968 ms\n",
      "Epoch 200 Loss 0.714484 Accuracy 0.899143 time taken 975 ms\n",
      "Epoch 210 Loss 0.686519 Accuracy 0.902286 time taken 952 ms\n",
      "Epoch 220 Loss 0.660771 Accuracy 0.903714 time taken 1045 ms\n",
      "Epoch 230 Loss 0.637003 Accuracy 0.904286 time taken 1186 ms\n",
      "Epoch 240 Loss 0.615008 Accuracy 0.905714 time taken 1081 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vanilla Gradient Descent with Data Parallelism\n",
    "\"\"\"\n",
    "\n",
    "#import the ThreadPool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "# from multiprocessing import Pool as ThreadPool\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-3\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\n",
    "#add additional hyperparameters related to the data parallelism\n",
    "threads_in_pool = 20\n",
    "parallel_batches = 16\n",
    "\n",
    "#create the thread pool\n",
    "pool = ThreadPool(processes=threads_in_pool) \n",
    "\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "vanilla_loss = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "    epochStartTime = time.time()\n",
    "    \n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "    \n",
    "    \n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(0, num_batches-parallel_batches, parallel_batches):\n",
    "        \n",
    "        #create the list of inputs for the pool threads\n",
    "        #this might look weird, but by creating a list of tuples, the input data can be easily given to\n",
    "        #each worker thread in the ThreadPool\n",
    "        minibatchGradientInputLists = []\n",
    "        for k in range(parallel_batches):\n",
    "            minibatchGradientInputLists.append((W1, W2, b1, b2, x_batches[j+k], y_batches[j+k]))\n",
    "        \n",
    "        # TODO: use the ThreadPool's map function to compute minibatch gradients in parallel.\n",
    "        gradientOutputs = pool.map(computeMinibatchGradientsTuple, minibatchGradientInputLists)\n",
    "        # grad_W1, grad_W2, grad_b1, grad_b2\n",
    "        '''\n",
    "        use the gradients to update weights and biases\n",
    "        '''\n",
    "        gradients = list(zip(*gradientOutputs))\n",
    "        # gradients = np.mean(gradients, axis=1)\n",
    "        W1 -= eta * np.mean(gradients[0], axis=0) # TODO: average (np.mean()) the W1 gradients we put into a list above.\n",
    "        W2 -= eta * np.mean(gradients[1], axis=0) # TODO: average (np.mean()) the W2 gradients we put into a list above.\n",
    "        b1 -= eta * np.mean(gradients[2], axis=0) # TODO: average (np.mean()) the b1 gradients we put into a list above.\n",
    "        b2 -= eta * np.mean(gradients[3], axis=0) # TODO: average (np.mean()) the b2 gradients we put into a list above.\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    vanilla_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "    \n",
    "    #find the time taken to compute the epoch\n",
    "    epochTimeTaken = (time.time() - epochStartTime) * 1000\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f time taken %d ms\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc, epochTimeTaken))\n",
    "\n",
    "#kill the pool so it doesn't hang around without getting garbage collected\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance assessment and questions\n",
    "\n",
    "Now that your data parallel implementation is finished, play around with the threads_in_pool and parallel_batches hyperparameters, and answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does the speed of the data parallel implementation compare to the non-parallelized version.\n",
    "\n",
    "\n",
    "2. Adjusting the threads_in_pool and parallel_batches hyperparameters, where do you see the most improvement in speed? When does increasing these hyperparameters stop making the computation faster?\n",
    "\n",
    "\n",
    "3. Section 3 of the paper discusses Generalization in the context of statistical accuracy. How does the generalization issue relate to the parallel_batches hyperparameter?\n",
    "\n",
    "\n",
    "4. Using a library like mpi4py, we could take the local, thread-parallel approach and do it in a true distributed environment. If the computeMinibatchGradients function was being run on different processors in a distributed system, what data would you have to send to the processors for each minibatch? What information would these distributed processors need to send back?\n",
    "\n",
    "\n",
    "5. As we discussed in class on Tuesday, model parallelism involves splitting up a network between processors such that different portions of the same layer might be computed on different processors. Knowing that the example network is comprised of two full-connected layers, what changes would you have to make to the code to be able to employ model parallelism. (Note, actually doing this would be an enormous amount of work, but think critically about which parts of the network would need to be rewritten to achieve model parallelism.) \n",
    "\n",
    "\n",
    "6. Pipeline parallelism involves splitting up a network between processors such that each processor is responsible for one or more contiguous operators. How might you change the example to perform pipeline-parallelism? Would this be easier to implement than model parallelism, or harder?\n",
    "\n",
    "\n",
    "7. If a pipeline-parallel network such as the one from the previous question was implemented, how would data quantization help improve performance in a distributed environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. My computer is slower when parallelizing the neural network. This is (probably) due to the fact that numpy operations are already parallelized, and there are enouhg threads that the overhead of switching between them outweighs the performance improvements. I'm sure it would be faster if I limited numpy to one thread.\n",
    "\n",
    "2. My Mac runs fastest with 1 thread. Second fastest is 2 threads, and performance decreases from there. This makes sense as my computer only has two cores. As the number of threads increases, the amount of overhead the OS increases, slowing down the computation. The other reason for the slowdown was discussed in (1).\n",
    "\n",
    "3. When there are more parallel batches, the performance of each batch increases. However, as the minibatch size increases, generalization decreases. This is because large batches \"can increase ... the gradient variance and learning rate\", and this can hinder convergence. Also, SGD becomes closer to GD as the minibatch size increases and this results in the loss of some SGD benefits. In this example, we should choose the parallel_batches parameter to optimize speed of computation and model accuracy.\n",
    "\n",
    "4. The weights of the model would have to be sent to each distributed computer, as well as the data each processor was working on. The gradients would also need to be sent back to each distributed cluster after the loss-reduce is performed at the end.\n",
    "\n",
    "5. Matrix multiplication can be done block-wise. I would spend time parallelizing the computation of each matrix-matrix product that corresponds to a fully-connected layer. Element-wise operators (add, subtract, application of an activation) could trivially be done in parallel.\n",
    "\n",
    "6. I would do one matmul (corresponding to a fc layer) on one processor, and the next on a second processor. This would probably require a decent amount of mental overhead to keep all the operations ordered, and to merge them after the batch is done. To actually change the example, I'd have to split computeMinibatchGradient into parts. I think this would be a little more difficult that model parallelism.\n",
    "\n",
    "7. Arithmetic operations would be much faster. There would also be much less data to move around, so weights/gradients could be transferred through the system much faster. The distributed environment could spend less time on data transfer and more time on computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (nn)",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
